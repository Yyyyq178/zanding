#å®šä¹‰äº† MAR çš„ä¸»ä½“æ¶æ„
from functools import partial

import numpy as np
from tqdm import tqdm
import scipy.stats as stats
import math
import torch
import torch.nn as nn
from torch.utils.checkpoint import checkpoint

from timm.models.vision_transformer import Block

from models.diffloss import DiffLoss



def get_1d_sincos_pos_embed_from_grid(embed_dim, pos):
    """
    embed_dim: æ¯ä¸ªä½ç½®çš„ç¼–ç ç»´åº¦
    pos: ä½ç½®åæ ‡åˆ—è¡¨ (M,)
    out: (M, D)
    """
    assert embed_dim % 2 == 0
    omega = np.arange(embed_dim // 2, dtype=np.float64)
    omega /= embed_dim / 2.
    omega = 1. / 10000**omega  # (D/2,)

    pos = pos.reshape(-1)  # (M,)
    out = np.einsum('m,d->md', pos, omega)  # (M, D/2), å¤–ç§¯

    emb_sin = np.sin(out) # (M, D/2)
    emb_cos = np.cos(out) # (M, D/2)

    emb = np.concatenate([emb_sin, emb_cos], axis=1)  # (M, D)
    return emb

def get_2d_sincos_pos_embed_from_grid(embed_dim, grid):
    assert embed_dim % 2 == 0
    # ä½¿ç”¨ä¸€åŠçš„ç»´åº¦ç¼–ç  Hï¼Œä¸€åŠçš„ç»´åº¦ç¼–ç  W
    emb_h = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[0])  # (H*W, D/2)
    emb_w = get_1d_sincos_pos_embed_from_grid(embed_dim // 2, grid[1])  # (H*W, D/2)
    emb = np.concatenate([emb_h, emb_w], axis=1) # (H*W, D)
    return emb

def get_2d_sincos_pos_embed(embed_dim, grid_size_hr, grid_size_lr):
    """
    ä¸º HR å’Œ LR ç”Ÿæˆå¯¹é½çš„ 2D sin-cos ä½ç½®ç¼–ç 
    grid_size_hr: int, ä¾‹å¦‚ 32 (å¯¹åº” 512åˆ†è¾¨ç‡)
    grid_size_lr: int, ä¾‹å¦‚ 8  (å¯¹åº” 128åˆ†è¾¨ç‡)
    """
    # 1. ç”Ÿæˆ HR çš„åæ ‡ç½‘æ ¼ (0 åˆ° grid_size_hr)
    grid_h = np.arange(grid_size_hr, dtype=np.float32)
    grid_w = np.arange(grid_size_hr, dtype=np.float32)
    grid = np.meshgrid(grid_w, grid_h)  # w åœ¨å‰
    grid = np.stack(grid, axis=0)
    grid = grid.reshape([2, 1, grid_size_hr, grid_size_hr])
    
    # ç”Ÿæˆ HR çš„ Embedding
    pos_embed_hr = get_2d_sincos_pos_embed_from_grid(embed_dim, grid)
    
    # 2. ç”Ÿæˆ LR çš„åæ ‡ç½‘æ ¼ (å…³é”®æ­¥éª¤ï¼)
    # æˆ‘ä»¬ä½¿ç”¨ linspace å°† LR çš„åæ ‡â€œæ‹‰ä¼¸â€è¦†ç›–åˆ°å’Œ HR ä¸€æ ·çš„ç©ºé—´èŒƒå›´
    # ä¾‹å¦‚: LR çš„ 0~7 åæ ‡ä¼šè¢«æ˜ å°„åˆ° HR çš„ 0~31 èŒƒå›´å†…
    grid_l_h = np.linspace(0, grid_size_hr - 1, grid_size_lr, dtype=np.float32)
    grid_l_w = np.linspace(0, grid_size_hr - 1, grid_size_lr, dtype=np.float32)
    grid_l = np.meshgrid(grid_l_w, grid_l_h)
    grid_l = np.stack(grid_l, axis=0)
    grid_l = grid_l.reshape([2, 1, grid_size_lr, grid_size_lr])
    
    # ç”Ÿæˆ LR çš„ Embedding
    pos_embed_lr = get_2d_sincos_pos_embed_from_grid(embed_dim, grid_l)
    
    # 3. æ‹¼æ¥: LR åœ¨å‰ (Buffer), HR åœ¨å
    # ç»“æœ shape: [seq_len + buffer_size, embed_dim]
    pos_embed = np.concatenate([pos_embed_lr, pos_embed_hr], axis=0)
    
    return pos_embed

def mask_by_order(mask_len, order, bsz, seq_len):
    masking = torch.zeros(bsz, seq_len).cuda()
    masking = torch.scatter(masking, dim=-1, index=order[:, :mask_len.long()], src=torch.ones(bsz, seq_len).cuda()).bool()
    return masking


class MAR(nn.Module):
    """ Masked Autoencoder with VisionTransformer backbone
    """
    def __init__(self, img_size=256, vae_stride=16, patch_size=1,
                 encoder_embed_dim=1024, encoder_depth=16, encoder_num_heads=16,
                 decoder_embed_dim=1024, decoder_depth=16, decoder_num_heads=16,
                 mlp_ratio=4., norm_layer=nn.LayerNorm,
                 vae_embed_dim=16,
                 mask_ratio_min=0.7,
                 #label_drop_prob=0.1,
                 #class_num=1000,
                 attn_dropout=0.1,
                 proj_dropout=0.1,
                 buffer_size=64,   #è¿™é‡Œéœ€è¦è®¾ç½®ä¸ºLRè½¬ç åçš„é•¿åº¦
                 diffloss_d=6,      #æ˜¾å­˜ç´§å¼ å¯ä»¥é™ä½
                 diffloss_w=1024,   #å¯ä»¥ä¸embed_dimä¿æŒä¸€è‡´
                 num_sampling_steps='100',
                 diffusion_batch_mul=1,         #å•å¡æœ€å¥½è®¾ä¸º1ï¼Œä¹‹å‰ä¸º4
                 grad_checkpointing=False,
                 ):
        super().__init__()

        # --------------------------------------------------------------------------
        # VAE and patchify specifics
        self.vae_embed_dim = vae_embed_dim

        self.img_size = img_size
        self.vae_stride = vae_stride
        self.patch_size = patch_size
        self.seq_h = self.seq_w = img_size // vae_stride // patch_size
        self.seq_len = self.seq_h * self.seq_w
        self.token_embed_dim = vae_embed_dim * patch_size**2
        self.grad_checkpointing = grad_checkpointing

        # --------------------------------------------------------------------------
        # Class Embedding
        #self.num_classes = class_num
        #self.class_emb = nn.Embedding(class_num, encoder_embed_dim)
        #self.label_drop_prob = label_drop_prob
        # Fake class embedding for CFG's unconditional generation
        #self.fake_latent = nn.Parameter(torch.zeros(1, encoder_embed_dim))

        # --------------------------------------------------------------------------
        # MAR variant masking ratio, a left-half truncated Gaussian centered at 100% masking ratio with std 0.25
        self.mask_ratio_generator = stats.truncnorm((mask_ratio_min - 1.0) / 0.25, 0, loc=1.0, scale=0.25)

        # --------------------------------------------------------------------------
        # MAR encoder specifics

        # æŠ•å½±å±‚ï¼šæŠŠ VAE çš„ 16 ç»´å‘é‡æ˜ å°„åˆ° Transformer çš„ 1024 ç»´
        self.z_proj = nn.Linear(self.token_embed_dim, encoder_embed_dim, bias=True)
        self.z_proj_ln = nn.LayerNorm(encoder_embed_dim, eps=1e-6)
        # ç¼“å†²åŒºå¤§å°ï¼šå®šä¹‰å‰ç¼€é•¿åº¦ (64)
        self.buffer_size = buffer_size
        # ä½ç½®ç¼–ç ï¼šè¿™æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„å‚æ•°è¡¨ï¼Œé•¿åº¦ = åºåˆ—é•¿åº¦(256) + ç¼“å†²åŒºé•¿åº¦(64) = 320ï¼Œç»´åº¦ = 1024
        self.encoder_pos_embed_learned = nn.Parameter(torch.zeros(1, self.seq_len + self.buffer_size, encoder_embed_dim))

        # Transformer Blocksï¼šå †å  16 å±‚
        self.encoder_blocks = nn.ModuleList([
            Block(encoder_embed_dim, encoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer,
                  proj_drop=proj_dropout, attn_drop=attn_dropout) for _ in range(encoder_depth)])
        self.encoder_norm = norm_layer(encoder_embed_dim)

        # --------------------------------------------------------------------------
        # MAR decoder specifics

        # æŠ•å½±å±‚ï¼šå¦‚æœ Encoder å’Œ Decoder ç»´åº¦ä¸ä¸€æ ·ï¼Œè¿™é‡Œè´Ÿè´£è½¬æ¢ (é€šå¸¸æ˜¯ä¸€æ ·çš„)
        self.decoder_embed = nn.Linear(encoder_embed_dim, decoder_embed_dim, bias=True)
        # mask_tokenï¼šè¿™æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„å‘é‡ï¼Œä»£è¡¨â€œæœªçŸ¥â€ï¼Œæ‰€æœ‰è¢«é®ä½çš„ä½ç½®ï¼Œéƒ½ä¼šå¡«å…¥è¿™ä¸ªå‘é‡
        self.mask_token = nn.Parameter(torch.zeros(1, 1, decoder_embed_dim))
        # Decoder ä½ç½®ç¼–ç 
        self.decoder_pos_embed_learned = nn.Parameter(torch.zeros(1, self.seq_len + self.buffer_size, decoder_embed_dim))

        # Transformer Blocksï¼šå †å  16 å±‚
        self.decoder_blocks = nn.ModuleList([
            Block(decoder_embed_dim, decoder_num_heads, mlp_ratio, qkv_bias=True, norm_layer=norm_layer,
                  proj_drop=proj_dropout, attn_drop=attn_dropout) for _ in range(decoder_depth)])

        self.decoder_norm = norm_layer(decoder_embed_dim)
        # Diffusion ä½ç½®ç¼–ç ï¼šè¿™æ˜¯ç»™ DiffLoss ç”¨çš„é¢å¤–ä½ç½®ä¿¡æ¯
        self.diffusion_pos_embed_learned = nn.Parameter(torch.zeros(1, self.seq_len, decoder_embed_dim))

        # --------------------------------------------------------------------------
        # Diffusion Loss
        # å®ä¾‹åŒ– DiffLoss æ¨¡å—ï¼Œå®ƒæ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å­ç½‘ç»œ (MLP æˆ– Transformer)
        self.diffloss = DiffLoss(
            target_channels=self.token_embed_dim,
            z_channels=decoder_embed_dim,
            width=diffloss_w,
            depth=diffloss_d,
            num_sampling_steps=num_sampling_steps,
            grad_checkpointing=grad_checkpointing
        )
        self.diffusion_batch_mul = diffusion_batch_mul

        # ğŸŸ¢ã€æ–°å¢ã€‘æœ€ç»ˆé¢„æµ‹å¤´ (ç”¨äº MSE Loss)(éœ€è¦åˆ é™¤)
        # æŠŠ Decoder çš„ 768 ç»´ç‰¹å¾æ˜ å°„å› 16 ç»´çš„ VAE Latent ç©ºé—´
        self.final_proj = nn.Linear(decoder_embed_dim, vae_embed_dim, bias=True)

        self.initialize_weights()

    def initialize_weights(self):
        # parameters
        #torch.nn.init.normal_(self.class_emb.weight, std=.02)
        #torch.nn.init.normal_(self.fake_latent, std=.02)
        torch.nn.init.normal_(self.mask_token, std=.02)
        #torch.nn.init.normal_(self.encoder_pos_embed_learned, std=.02)
        #torch.nn.init.normal_(self.decoder_pos_embed_learned, std=.02)
        #torch.nn.init.normal_(self.diffusion_pos_embed_learned, std=.02)
        grid_size_hr = int(self.seq_len**0.5) 
        grid_size_lr = int(self.buffer_size**0.5)

        pos_embed = get_2d_sincos_pos_embed(
            self.encoder_pos_embed_learned.shape[-1], # embed_dim (e.g. 768 or 1024)
            grid_size_hr, 
            grid_size_lr
        )
        # èµ‹å€¼ç»™ Encoder å’Œ Decoder
        # unsqueeze(0) æ˜¯ä¸ºäº†å¢åŠ  Batch ç»´åº¦: [1, Total_Len, Dim]
        self.encoder_pos_embed_learned.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))
        self.decoder_pos_embed_learned.data.copy_(torch.from_numpy(pos_embed).float().unsqueeze(0))

        # èµ‹å€¼ç»™ Diffusion (åªå– HR éƒ¨åˆ†ï¼Œå»æ‰ buffer)
        pos_embed_hr_only = pos_embed[self.buffer_size:, :] 
        self.diffusion_pos_embed_learned.data.copy_(torch.from_numpy(pos_embed_hr_only).float().unsqueeze(0))

        # 3. åˆå§‹åŒ–é€šç”¨å±‚ (Linear, LayerNorm)
        # è¿™ä¼šé€’å½’åˆå§‹åŒ–åŒ…æ‹¬ DiffLoss åœ¨å†…çš„æ‰€æœ‰å±‚
        self.apply(self._init_weights)
    def _init_weights(self, m):
        # åˆå§‹åŒ–å…¨è¿æ¥å±‚å’Œå½’ä¸€åŒ–å±‚çš„biaså’Œweight
        if isinstance(m, nn.Linear):
            # we use xavier_uniform following official JAX ViT:
            torch.nn.init.xavier_uniform_(m.weight)
            if isinstance(m, nn.Linear) and m.bias is not None:
                nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.LayerNorm):
            if m.bias is not None:
                nn.init.constant_(m.bias, 0)
            if m.weight is not None:
                nn.init.constant_(m.weight, 1.0)

    def patchify(self, x):
        # åˆ‡ç‰‡ï¼Œå›¾ç‰‡->åºåˆ—
        bsz, c, h, w = x.shape
        p = self.patch_size
        h_, w_ = h // p, w // p

        x = x.reshape(bsz, c, h_, p, w_, p)
        x = torch.einsum('nchpwq->nhwcpq', x)
        x = x.reshape(bsz, h_ * w_, c * p ** 2)
        return x  # [n, l, d]   è¾“å‡ºå½¢çŠ¶: [Batch, Seq_Len=256, Dim=16]

    def unpatchify(self, x):
        # åå‘é‡å¡‘
        bsz = x.shape[0]
        p = self.patch_size
        c = self.vae_embed_dim
        h_, w_ = self.seq_h, self.seq_w

        x = x.reshape(bsz, h_, w_, c, p, p)
        x = torch.einsum('nhwcpq->nchpwq', x)
        x = x.reshape(bsz, c, h_ * p, w_ * p)
        return x  # [n, c, h, w]

    def sample_orders(self, bsz):
        # generate a batch of random generation orders
        orders = []
        for _ in range(bsz):
            order = np.array(list(range(self.seq_len)))
            np.random.shuffle(order)
            orders.append(order)
        orders = torch.Tensor(np.array(orders)).cuda().long()
        return orders

    def random_masking(self, x, orders):
        # generate token mask
        bsz, seq_len, embed_dim = x.shape
        mask_rate = self.mask_ratio_generator.rvs(1)[0]
        num_masked_tokens = int(np.ceil(seq_len * mask_rate))
        mask = torch.zeros(bsz, seq_len, device=x.device)
        # æ‰§è¡Œé®æŒ¡ (æŠŠæŒ‡å®šä½ç½®è®¾ä¸º 1/Masked)
        mask = torch.scatter(mask, dim=-1, index=orders[:, :num_masked_tokens],
                             src=torch.ones(bsz, seq_len, device=x.device))
        return mask

    def forward_mae_encoder(self, x_hr, mask, x_lr):
        # x: [Batch, Seq_Len=256, Dim=16] (VAE è¾“å‡ºçš„ token)
        # mask: [Batch, Seq_Len=256] (0=å¯è§, 1=é®æŒ¡)
        # x_lr: LR tokensï¼ˆæ¥è‡ªVAEç¼–ç ï¼Œç»´åº¦ 16ï¼‰
        hr_embedding = self.z_proj(x_hr)
        lr_embedding = self.z_proj(x_lr)
        bsz, seq_len, embed_dim = hr_embedding.shape

        # concat buffer
        # x = torch.cat([torch.zeros(bsz, self.buffer_size, embed_dim, device=x.device), x], dim=1)
        x = torch.cat([lr_embedding, hr_embedding], dim=1)
        # ç»™bufferæ‰“ä¸Š0ï¼Œè¡¨ç¤ºæ°¸è¿œå¯è§
        mask_with_buffer = torch.cat([torch.zeros(x.size(0), self.buffer_size, device=x.device), mask], dim=1)

        # æ³¨å…¥ç±»åˆ«æ¡ä»¶ (CFG æŠ€å·§)
        #if self.training:
            # è®­ç»ƒæ—¶æœ‰ 10% æ¦‚ç‡æŠŠç±»åˆ« Embedding æ›¿æ¢æˆâ€œå‡æ ‡ç­¾â€(fake_latent)
            #drop_latent_mask = torch.rand(bsz) < self.label_drop_prob
            #drop_latent_mask = drop_latent_mask.unsqueeze(-1).cuda().to(x.dtype)
            #class_embedding = drop_latent_mask * self.fake_latent + (1 - drop_latent_mask) * class_embedding

        # æŠŠç±»åˆ«å‘é‡å¡«æ»¡é‚£ 64 ä¸ª buffer ç©ºä½ï¼ˆé‡ç‚¹ä¿®æ”¹è¿™ä¸€æ­¥ï¼‰
        #x[:, :self.buffer_size] = class_embedding.unsqueeze(1)

        # encoder position embedding
        # è¿™ä¸€æ­¥èµ‹äºˆäº†æ¯ä¸ª token ç©ºé—´ä½ç½®ä¿¡æ¯
        x = x + self.encoder_pos_embed_learned
        x = self.z_proj_ln(x)

        # dropping
        # åªå–å‡º mask=0 çš„ä½ç½®ã€‚è¿™ä¸€æ­¥æå¤§åœ°å‡å°‘äº†è®¡ç®—é‡
        x = x[(1-mask_with_buffer).nonzero(as_tuple=True)].reshape(bsz, -1, embed_dim)

        # apply Transformer blocks
        # æ­¤æ—¶ x çš„é•¿åº¦å˜çŸ­äº† (æ¯”å¦‚ä» 320 å˜æˆäº† 64+å‡ åä¸ªå¯è§token)
        for block in self.encoder_blocks:
            if self.grad_checkpointing and not torch.jit.is_scripting():         
                x = checkpoint(block, x)
            else:
                x = block(x)
        x = self.encoder_norm(x)

        return x

    def forward_mae_decoder(self, x, mask):
        # x: Encoder çš„è¾“å‡º (åªæœ‰å¯è§éƒ¨åˆ† + Buffer)
        # mask: åŸå§‹çš„é®æŒ¡æ©ç 
        x = self.decoder_embed(x)
        # é‡å»ºå¸¦æœ‰ buffer çš„å®Œæ•´ mask
        mask_with_buffer = torch.cat([torch.zeros(x.size(0), self.buffer_size, device=x.device), mask], dim=1)

        # pad mask tokensï¼ˆå¡«è¡¥ç©ºç¼ºï¼‰
        # å…ˆé€ ä¸€ä¸ªå…¨æ˜¯ä¸å¯è§çš„åº•æ¿ï¼Œå½¢çŠ¶æ˜¯å®Œæ•´çš„[Batch, 320, dim]
        mask_tokens = self.mask_token.repeat(mask_with_buffer.shape[0], mask_with_buffer.shape[1], 1).to(x.dtype)
        x_after_pad = mask_tokens.clone()
        # æŠŠå¯è§ç‰¹å¾å¡«å›å®ƒåŸæ¥çš„ä½ç½®
        x_after_pad[(1 - mask_with_buffer).nonzero(as_tuple=True)] = x.reshape(x.shape[0] * x.shape[1], x.shape[2])

        # decoder position embedding
        x = x_after_pad + self.decoder_pos_embed_learned

        # apply Transformer blocks
        # Decoder å¤„ç†çš„æ˜¯å®Œæ•´çš„é•¿åºåˆ— (320)
        # å®ƒè¦åˆ©ç”¨å¯è§ä¿¡æ¯ï¼Œå»â€œçŒœâ€ mask_token é‚£ä¸ªä½ç½®åº”è¯¥æ˜¯ä»€ä¹ˆ
        if self.grad_checkpointing and not torch.jit.is_scripting():
            for block in self.decoder_blocks:
                x = checkpoint(block, x)
        else:
            for block in self.decoder_blocks:
                x = block(x)
        x = self.decoder_norm(x)

        # ç§»é™¤buffer
        x = x[:, self.buffer_size:]
        # åŠ ä¸Šdiffusionä½ç½®ç¼–ç 
        x = x + self.diffusion_pos_embed_learned
        return x

    def forward_loss(self, z, target, mask):
        # z: Decoder é¢„æµ‹å‡ºçš„ Latent ç‰¹å¾ [Batch, Seq_Len, Dim]
        # target: çœŸå®çš„ Latent ç‰¹å¾ (Ground Truth) [Batch, Seq_Len, Dim]
        # mask: å½“å‰çš„é®æŒ¡æ©ç  [Batch, Seq_Len]  

        #bsz, seq_len, _ = target.shape#æ–°æ³¨é‡Šçš„ï¼Œéœ€è¦è¿˜åŸ

        # æ¯ä¸€å¼ å›¾ç‰‡åœ¨ä¸€æ¬¡ Forward ä¸­åŒæ—¶å­¦ä¹  4 ä¸ªä¸åŒçš„æ‰©æ•£æ—¶é—´æ­¥ï¼ˆdiffusion_batch_mulï¼‰
        #target = target.reshape(bsz * seq_len, -1).repeat(self.diffusion_batch_mul, 1)#æ–°æ³¨é‡Šçš„ï¼Œéœ€è¦è¿˜åŸ
        #z = z.reshape(bsz*seq_len, -1).repeat(self.diffusion_batch_mul, 1)#æ–°æ³¨é‡Šçš„ï¼Œéœ€è¦è¿˜åŸ
        #mask = mask.reshape(bsz*seq_len).repeat(self.diffusion_batch_mul)#æ–°æ³¨é‡Šçš„ï¼Œéœ€è¦è¿˜åŸ

        # z æ˜¯æ¡ä»¶ (Condition)ï¼Œtarget æ˜¯è¦åŠ å™ªçš„æ•°æ® (x_start)
        #loss = self.diffloss(z=z, target=target, mask=mask)#æ–°æ³¨é‡Šçš„ï¼Œéœ€è¦è¿˜åŸ

        ### ä¸‹é¢æ˜¯æµ‹è¯•ç”¨çš„ä»£ç ï¼Œéœ€è¦åˆ é™¤
        # MSE Loss é€»è¾‘
        # è®¡ç®—æ¯ä¸ªå…ƒç´ çš„å¹³æ–¹å·®: (Pred - GT)^2
        z = self.final_proj(z)
        loss = (z - target) ** 2 
        
        # åœ¨ç‰¹å¾ç»´åº¦(Dim)ä¸Šå–å¹³å‡ -> [Batch, Seq_Len]
        loss = loss.mean(dim=-1)
        
        # åªè®¡ç®—è¢«é®æŒ¡éƒ¨åˆ†(Mask=1)çš„ Loss
        # sum() æ˜¯æ€»è¯¯å·®ï¼Œmask.sum() æ˜¯è¢«é®æŒ¡çš„ Token æ€»æ•°
        # åŠ ä¸€ä¸ªæå°å€¼ 1e-6 é˜²æ­¢é™¤ä»¥ 0
        loss = (loss * mask).sum() / (mask.sum() + 1e-6)
        return loss

    def forward(self, x_hr, x_lr):
        # x_hr: HR Latents [B, 16, 16, 16]
        # x_lr: LR Latents [B, 16, 32, 32]
        # class embed
        #class_embedding = self.class_emb(x_lr)

        # patchify and mask (drop) tokens
        # åˆ‡ç‰‡åŒ–
        lr_tokens = self.patchify(x_lr)
        hr_tokens = self.patchify(x_hr)
        # å¤‡ä»½groundtruthï¼ˆgt_latentsï¼‰
        gt_latents = hr_tokens.clone().detach()
        # ç”Ÿæˆéšæœºæ©ç 
        orders = self.sample_orders(bsz=hr_tokens.size(0))
        mask = self.random_masking(hr_tokens, orders)

        # mae encoder
        x = self.forward_mae_encoder(hr_tokens, mask, lr_tokens)

        # mae decoder
        z = self.forward_mae_decoder(x, mask)

        # diffloss
        loss = self.forward_loss(z=z, target=gt_latents, mask=mask)

        return loss

    def sample_tokens(self, bsz, num_iter=64, cfg=1.0, cfg_schedule="linear", x_lr=None, temperature=1.0, progress=False):
        #num_iterï¼šè‡ªå›å½’è¿­ä»£æ¬¡æ•°ï¼ˆå®˜æ–¹æ¨èè®¾ç½®256ï¼‰
        # init and sample generation orders
        # åˆå§‹åŒ–æ©ç ï¼šå…¨ä¸º 1 (ä»£è¡¨å…¨å›¾è¢«é®æŒ¡/æœªçŸ¥)
        mask = torch.ones(bsz, self.seq_len).cuda()
        # åˆå§‹åŒ– Tokenï¼šå…¨ä¸º 0 (ç”»å¸ƒæ˜¯é»‘çš„)
        tokens = torch.zeros(bsz, self.seq_len, self.token_embed_dim).cuda()
        # ç”Ÿæˆéšæœºé¡ºåºï¼šå†³å®šå…ˆç”»å“ªå„¿ï¼Œåç”»å“ªå„¿
        orders = self.sample_orders(bsz)

        # å¦‚æœæ¨ç†æ—¶ä¼ å…¥äº† x_lrï¼ˆLatentå½¢å¼ï¼‰,å…ˆæŠŠå®ƒå˜æˆtokens
        if x_lr is not None:
            lr_tokens = self.patchify(x_lr)
        else:
            raise ValueError("Super-Resolution requires LR input!")

        indices = list(range(num_iter))
        if progress:
            indices = tqdm(indices)
        # generate latents
        for step in indices:
            cur_tokens = tokens.clone()

            # class embedding and CFG
            # å¦‚æœå¯ç”¨ CFG (cfg != 1.0)ï¼Œæˆ‘ä»¬éœ€è¦æ„é€ â€œåŒå€ Batchâ€
            #if x_lr is not None:
            #    class_embedding = self.class_emb(x_lr)
            #else:
            #    class_embedding = self.fake_latent.repeat(bsz, 1)
            #if not cfg == 1.0:
            #    tokens = torch.cat([tokens, tokens], dim=0)
            #    class_embedding = torch.cat([class_embedding, self.fake_latent.repeat(bsz, 1)], dim=0)
            #    mask = torch.cat([mask, mask], dim=0)

            # mae encoder
            x = self.forward_mae_encoder(tokens, mask, lr_tokens)

            # mae decoder
            z = self.forward_mae_decoder(x, mask)

            # mask ratio for the next round, following MaskGIT and MAGE.
            mask_ratio = np.cos(math.pi / 2. * (step + 1) / num_iter)
            mask_len = torch.Tensor([np.floor(self.seq_len * mask_ratio)]).cuda()

            # masks out at least one for the next iteration
            mask_len = torch.maximum(torch.Tensor([1]).cuda(),
                                     torch.minimum(torch.sum(mask, dim=-1, keepdims=True) - 1, mask_len))

            # get masking for next iteration and locations to be predicted in this iteration
            mask_next = mask_by_order(mask_len[0], orders, bsz, self.seq_len)
            if step >= num_iter - 1:
                mask_to_pred = mask[:bsz].bool()
            else:
                mask_to_pred = torch.logical_xor(mask[:bsz].bool(), mask_next.bool())
            mask = mask_next
            if not cfg == 1.0:
                mask_to_pred = torch.cat([mask_to_pred, mask_to_pred], dim=0)

            # sample token latents for this step
            z = z[mask_to_pred.nonzero(as_tuple=True)]
            ##469-479ä¸ºæµ‹è¯•æ—¶æ³¨é‡Šæ‰çš„éƒ¨åˆ†ï¼Œéœ€è¦æ¢å¤ï¼Œ480,481éœ€è¦åˆ é™¤

            # # cfg schedule follow Muse
            # if cfg_schedule == "linear":
            #     cfg_iter = 1 + (cfg - 1) * (self.seq_len - mask_len[0]) / self.seq_len
            # elif cfg_schedule == "constant":
            #     cfg_iter = cfg
            # else:
            #     raise NotImplementedError
            # sampled_token_latent = self.diffloss.sample(z, temperature, cfg_iter)
            # if not cfg == 1.0:
            #     sampled_token_latent, _ = sampled_token_latent.chunk(2, dim=0)  # Remove null class samples
            #     mask_to_pred, _ = mask_to_pred.chunk(2, dim=0)
            z = self.final_proj(z)
            sampled_token_latent = z

            cur_tokens[mask_to_pred.nonzero(as_tuple=True)] = sampled_token_latent
            tokens = cur_tokens.clone()
        
        # unpatchify
        tokens = self.unpatchify(tokens)
        return tokens


def mar_base(**kwargs):
    model = MAR(
        encoder_embed_dim=768, encoder_depth=12, encoder_num_heads=12,
        decoder_embed_dim=768, decoder_depth=12, decoder_num_heads=12,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def mar_large(**kwargs):
    model = MAR(
        encoder_embed_dim=1024, encoder_depth=16, encoder_num_heads=16,
        decoder_embed_dim=1024, decoder_depth=16, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model


def mar_huge(**kwargs):
    model = MAR(
        encoder_embed_dim=1280, encoder_depth=20, encoder_num_heads=16,
        decoder_embed_dim=1280, decoder_depth=20, decoder_num_heads=16,
        mlp_ratio=4, norm_layer=partial(nn.LayerNorm, eps=1e-6), **kwargs)
    return model
