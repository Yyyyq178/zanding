#å°è£…äº†å…·ä½“çš„è®­ç»ƒå’Œè¯„ä¼°é€»è¾‘ã€‚
import math
import sys
from typing import Iterable

import torch

import util.misc as misc
import util.lr_sched as lr_sched
import torch.nn.functional as F
from models.vae import DiagonalGaussianDistribution
#import torch_fidelity
import pyiqa
import shutil
import cv2
import numpy as np
import os
import copy
import time

os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"

def update_ema(target_params, source_params, rate=0.99):
    """
    Update target parameters to be closer to those of source parameters using
    an exponential moving average.

    :param target_params: the target parameter sequence.
    :param source_params: the source parameter sequence.
    :param rate: the EMA rate (closer to 1 means slower).
    """
    for targ, src in zip(target_params, source_params):
        targ.detach().mul_(rate).add_(src, alpha=1 - rate)


def train_one_epoch(model, vae,
                    model_params, ema_params,
                    data_loader: Iterable, optimizer: torch.optim.Optimizer,
                    device: torch.device, epoch: int, loss_scaler,
                    log_writer=None,
                    args=None):
    model.train(True)
    metric_logger = misc.MetricLogger(delimiter="  ")
    metric_logger.add_meter('lr', misc.SmoothedValue(window_size=1, fmt='{value:.6f}'))
    header = 'Epoch: [{}]'.format(epoch)
    print_freq = 20

    optimizer.zero_grad()

    if log_writer is not None:
        print('log_dir: {}'.format(log_writer.log_dir))
    
    num_steps_per_epoch = len(data_loader)
    if args.steps_per_epoch > 0 and args.steps_per_epoch < len(data_loader):
        num_steps_per_epoch = args.steps_per_epoch

    for data_iter_step, (samples_hr, samples_lr, _) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):

        # å¦‚æœå½“å‰æ­¥æ•°è¾¾åˆ°äº†æˆ‘ä»¬è®¾å®šçš„é™åˆ¶ï¼Œç›´æ¥å¼ºè¡Œç»“æŸè¿™ä¸€è½®ï¼Œè¿›å…¥ Validation
        if args.steps_per_epoch > 0 and data_iter_step >= args.steps_per_epoch:
            break

        # we use a per iteration (instead of per epoch) lr scheduler
        lr_sched.adjust_learning_rate(optimizer, data_iter_step / num_steps_per_epoch + epoch, args)

        # æŠŠæ•°æ®ç§»åˆ° GPU
        samples_hr = samples_hr.to(device, non_blocking=True)
        samples_lr = samples_lr.to(device, non_blocking=True)
        #samples_lr = samples_lr.to(device, non_blocking=True)

        with torch.no_grad():
            if args.use_cached:
                 raise NotImplementedError("Cached mode not supported for SR yet.")
            else:
                # === ç¼–ç  HR ===
                posterior_hr = vae.encode(samples_hr)
                x_hr = posterior_hr.sample().mul_(0.2325) 

                # === ç¼–ç  LR ===
                posterior_lr = vae.encode(samples_lr)
                x_lr = posterior_lr.sample().mul_(0.2325)

        # forward
        with torch.amp.autocast('cuda'):
            # ä¼ å…¥æ¨¡å‹
            loss = model(x_hr, x_lr)

        loss_value = loss.item()

        if not math.isfinite(loss_value):
            print("Loss is {}, stopping training".format(loss_value))
            sys.exit(1)

        loss_scaler(loss, optimizer, clip_grad=args.grad_clip, parameters=model.parameters(), update_grad=True)
        optimizer.zero_grad()

        torch.cuda.synchronize()

        update_ema(ema_params, model_params, rate=args.ema_rate)

        metric_logger.update(loss=loss_value)

        lr = optimizer.param_groups[0]["lr"]
        metric_logger.update(lr=lr)

        loss_value_reduce = misc.all_reduce_mean(loss_value)
        if log_writer is not None:
            """ We use epoch_1000x as the x-axis in tensorboard.
            This calibrates different curves when batch size changes.
            """
            epoch_1000x = int((data_iter_step / len(data_loader) + epoch) * 1000)
            log_writer.add_scalar('train_loss', loss_value_reduce, epoch_1000x)
            log_writer.add_scalar('lr', lr, epoch_1000x)

    # gather the stats from all processes
    metric_logger.synchronize_between_processes()
    print("Averaged stats:", metric_logger)
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}


def evaluate(model_without_ddp, vae, ema_params, args, epoch, batch_size=16, log_writer=None, cfg=1.0,
             use_ema=True, data_loader=None):
    torch.cuda.empty_cache()
    model_without_ddp.eval()
    #num_steps = args.num_images // (batch_size * misc.get_world_size()) + 1
    save_folder = os.path.join(args.output_dir, "ariter{}-diffsteps{}-temp{}-{}cfg{}-image_num{}".format(args.num_iter,
                                                                                                     args.num_sampling_steps,
                                                                                                     args.temperature,
                                                                                                     args.cfg_schedule,
                                                                                                     cfg,
                                                                                                     args.num_images))
    if use_ema:
        save_folder = save_folder + "_ema"
    if args.evaluate:
        save_folder = save_folder + "_evaluate"
    print("Save to:", save_folder)

    if misc.get_rank() == 0:
        os.makedirs(save_folder, exist_ok=True)
    # switch to ema params
    if use_ema:
        model_state_dict = copy.deepcopy(model_without_ddp.state_dict())
        ema_state_dict = copy.deepcopy(model_without_ddp.state_dict())
        for i, (name, _value) in enumerate(model_without_ddp.named_parameters()):
            assert name in ema_state_dict
            ema_state_dict[name] = ema_params[i]
        print("Switch to ema")
        model_without_ddp.load_state_dict(ema_state_dict)

    used_time = 0

    gen_img_cnt = 0

    if data_loader is None:
        print("No data loader provided for evaluation, skipping.")
        return
    
    # --- åˆå§‹åŒ–è¯„ä»·æŒ‡æ ‡ (åŠ è½½æ¨¡å‹åˆ° GPU) ---
    print("Loading metrics models (PSNR/SSIM/LPIPS)...")
    # è¿™é‡Œçš„ device='cuda' å‡è®¾ä½ ä¸€å®šç”¨ GPUã€‚LPIPS è®¡ç®—è¾ƒæ…¢ï¼Œå¦‚æœä¸å…³å¿ƒå¯ä»¥æ³¨é‡Šæ‰ã€‚
    psnr_metric = pyiqa.create_metric('psnr', device='cuda')
    ssim_metric = pyiqa.create_metric('ssim', device='cuda')
    lpips_metric = pyiqa.create_metric('lpips', device='cuda') 
    
    # ä½¿ç”¨ misc.MetricLogger æ¥è‡ªåŠ¨ç®¡ç†æ‰€æœ‰ GPU ä¸Šçš„å¹³å‡åˆ†è®¡ç®—
    metric_logger = misc.MetricLogger(delimiter="  ")
    # -------------------------------------------

    print(f"Start evaluation on {len(data_loader)} batches...")
    
    # å¼€å§‹éå†éªŒè¯é›† (HR, LR)
    for i, (imgs_hr, imgs_lr, filenames) in enumerate(data_loader):
        
        # 1. å‡†å¤‡æ•°æ®
        imgs_hr = imgs_hr.cuda(non_blocking=True)
        #imgs_lr = imgs_lr.cuda(non_blocking=True)
        imgs_lr = imgs_lr.cuda(non_blocking=True)
        # ä¸ºäº†èŠ‚çœæ—¶é—´ï¼Œåªè·‘å‰ 5 ä¸ª batch çœ‹æ•ˆæœ
        if i >= 5: 
            print("Finished 5 batches preview, stopping evaluation.")
            break 
        # 2. ç¼–ç  LR (ä½œä¸ºæ¡ä»¶)
        with torch.no_grad():
            posterior_lr = vae.encode(imgs_lr)
            x_lr = posterior_lr.sample().mul_(0.2325) 

        # è®¡ç®—ç›®æ ‡ Token æ•°é‡ (HR Latent çš„é•¿åº¦)
        h_hr, w_hr = imgs_hr.shape[-2:]
        feat_h, feat_w = h_hr // 16, w_hr // 16 # å‡è®¾ stride=16
        target_seq_len = feat_h * feat_w

        # 3. ç”Ÿæˆ (Inference)
        with torch.no_grad():
            with torch.amp.autocast('cuda'):
                # è°ƒç”¨ sample_tokensï¼Œä¼ å…¥ x_lr
                sampled_tokens = model_without_ddp.sample_tokens(
                    bsz=imgs_lr.shape[0], 
                    num_iter=args.num_iter, 
                    cfg=cfg,
                    cfg_schedule=args.cfg_schedule, 
                    x_lr=x_lr,
                    temperature=args.temperature,
                    target_seq_len=target_seq_len  
                )
            
                # è§£ç ç”Ÿæˆçš„ Token å˜å›å›¾ç‰‡
                sampled_images = vae.decode(sampled_tokens / 0.2325)

        # 1. æ•°æ®é¢„å¤„ç†ï¼šå°†èŒƒå›´ä» [-1, 1] è½¬æ¢åˆ° [0, 1]
        # æ³¨æ„ï¼špyiqa æœŸæœ›è¾“å…¥æ˜¯ [0, 1] çš„ float32 Tensor
        sr_tensor = (sampled_images + 1) / 2
        sr_tensor = sr_tensor.clamp(0, 1).float()  # æˆªæ–­é˜²æ­¢è¶Šç•Œ
        
        hr_tensor = (imgs_hr + 1) / 2
        hr_tensor = hr_tensor.clamp(0, 1).float()

        # 2. è®¡ç®—å½“å‰ batch çš„åˆ†æ•°
        with torch.no_grad():
            # pyiqa ä¼šè¿”å›è¿™ä¸ª batch çš„å¹³å‡åˆ† (scalar tensor)
            batch_psnr = psnr_metric(sr_tensor, hr_tensor)
            batch_ssim = ssim_metric(sr_tensor, hr_tensor)
            batch_lpips = lpips_metric(sr_tensor, hr_tensor)

        # 3. è®°å½•åˆ†æ•° (MetricLogger ä¼šè‡ªåŠ¨å¤„ç†ç´¯åŠ å’Œå¹³æ»‘)
        metric_logger.update(psnr=batch_psnr.mean().item())
        metric_logger.update(ssim=batch_ssim.mean().item())
        metric_logger.update(lpips=batch_lpips.item())
        # 4. ä¿å­˜å›¾ç‰‡ (è°ƒç”¨è¾…åŠ©å‡½æ•°)
        if misc.get_rank() == 0: # åªåœ¨ä¸»è¿›ç¨‹ä¿å­˜
            save_comparison_images(sampled_images, imgs_hr, imgs_lr, save_folder, i)

    torch.distributed.barrier()
    time.sleep(10)

    # back to no ema
    if use_ema:
        print("Switch back from ema")
        model_without_ddp.load_state_dict(model_state_dict)

    # 1. åŒæ­¥æ‰€æœ‰æ˜¾å¡çš„ç»Ÿè®¡æ•°æ® (è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå¦åˆ™ä½ çœ‹åˆ°çš„åªæœ‰ä¸»å¡çš„åˆ†æ•°)
    metric_logger.synchronize_between_processes()
    
    print("Averaged Validation stats:", metric_logger)

    # å°†æŒ‡æ ‡ä¿å­˜åˆ° TXT æ–‡ä»¶
    if misc.get_rank() == 0:
        # å®šä¹‰ txt æ–‡ä»¶è·¯å¾„ (ä¿å­˜åœ¨æœ¬æ¬¡è¯„ä¼°çš„ save_folder ä¸‹)
        txt_path = os.path.join(save_folder, "metrics_results.txt")
        
        with open(txt_path, "w") as f:
            f.write(f"Evaluation Results (Epoch {epoch}):\n")
            f.write("=" * 30 + "\n")
            # éå† logger ä¸­çš„æ‰€æœ‰æŒ‡æ ‡å†™å…¥
            for name, meter in metric_logger.meters.items():
                f.write(f"{name}: {meter.global_avg:.4f}\n")
            f.write("=" * 30 + "\n")
            f.write(f"Full stats: {metric_logger}\n")
            
        print(f"âœ… Metrics saved to: {txt_path}")
    
    # 2. å¦‚æœé…ç½®äº† Tensorboardï¼Œå†™å…¥éªŒè¯é›†æŒ‡æ ‡
    if log_writer is not None:
        # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ä½ åœ¨ç¬¬äºŒæ­¥ä¸­å·²ç»åƒ metric_logger.update(psnr=...) é‚£æ ·æ·»åŠ äº†è¿™äº› key
        if 'psnr' in metric_logger.meters:
            log_writer.add_scalar('val/psnr', metric_logger.meters['psnr'].global_avg, epoch)
        if 'ssim' in metric_logger.meters:
            log_writer.add_scalar('val/ssim', metric_logger.meters['ssim'].global_avg, epoch)
        if 'lpips' in metric_logger.meters:    
            log_writer.add_scalar('val/lpips', metric_logger.meters['lpips'].global_avg, epoch)
    # è¿”å›ç»Ÿè®¡ç»“æœï¼Œæ–¹ä¾¿å¤–éƒ¨è°ƒç”¨
    return {k: meter.global_avg for k, meter in metric_logger.meters.items()}

    torch.distributed.barrier()
    time.sleep(10)


def cache_latents(vae,
                  data_loader: Iterable,
                  device: torch.device,
                  args=None):
    metric_logger = misc.MetricLogger(delimiter="  ")
    header = 'Caching: '
    print_freq = 20

    for data_iter_step, (samples, _, paths) in enumerate(metric_logger.log_every(data_loader, print_freq, header)):

        samples = samples.to(device, non_blocking=True)

        with torch.no_grad():
            posterior = vae.encode(samples)
            moments = posterior.parameters
            posterior_flip = vae.encode(samples.flip(dims=[3]))
            moments_flip = posterior_flip.parameters

        for i, path in enumerate(paths):
            save_path = os.path.join(args.cached_path, path + '.npz')
            os.makedirs(os.path.dirname(save_path), exist_ok=True)
            np.savez(save_path, moments=moments[i].cpu().numpy(), moments_flip=moments_flip[i].cpu().numpy())

        if misc.is_dist_avail_and_initialized():
            torch.cuda.synchronize()

    return
def save_comparison_images(sr, hr, lr, save_folder, batch_idx):
    # è¾…åŠ©å‡½æ•°ï¼šæŠŠ Tensor è½¬ä¸º numpy å›¾ç‰‡
    def process(x):
        # --- ğŸŸ¢ã€æ–°å¢ã€‘ç¬¬ä¸€æ­¥ï¼šè¯Šæ–­æ˜¯å¦ç‚¸äº† ---
        if torch.isnan(x).any() or torch.isinf(x).any():
            print(f"\nâš ï¸ è­¦å‘Šï¼šåœ¨ç”Ÿæˆç»“æœä¸­æ£€æµ‹åˆ° NaN æˆ– Infï¼(Batch {batch_idx})")
            print(f"    ç»Ÿè®¡: Min={x.min().item():.4f}, Max={x.max().item():.4f}, Mean={x.mean().item():.4f}")
            print("    -> è¿™è¯´æ˜æ¨¡å‹è®­ç»ƒä¸ç¨³å®šï¼ˆå­¦ä¹ ç‡å¤ªå¤§ï¼‰ï¼Œå»ºè®®é™ä½ blrã€‚")
        
        # --- æ­£å¸¸å¤„ç†æµç¨‹ ---
        # åå½’ä¸€åŒ– (ä» -1~1 æ˜ å°„å› 0~1)
        x = (x + 1) / 2
        
        # å¦‚æœæ˜¯ NaNï¼Œå˜æˆ 0 (é»‘)ï¼›å¦‚æœæ˜¯ Infï¼Œå˜æˆ 1 (ç™½)
        x = torch.nan_to_num(x, nan=0.0, posinf=1.0, neginf=0.0)
        
        # é™åˆ¶èŒƒå›´å¹¶è½¬ numpy
        return x.clamp(0, 1).cpu().permute(0, 2, 3, 1).numpy()
    
    sr_np = process(sr)
    hr_np = process(hr)
    
    # æŠŠ LR æ”¾å¤§åˆ°å’Œ SR ä¸€æ ·å¤§
    lr_upscaled = F.interpolate(lr, size=sr.shape[-2:], mode='nearest')
    lr_np = process(lr_upscaled)
    
    # éå†è¿™ä¸ª batch é‡Œçš„æ¯å¼ å›¾å¹¶ä¿å­˜
    for j in range(sr_np.shape[0]):
        combined = np.concatenate([lr_np[j], sr_np[j], hr_np[j]], axis=1)
        combined = (combined * 255).astype(np.uint8)
        combined = cv2.cvtColor(combined, cv2.COLOR_RGB2BGR)
        
        filename = os.path.join(save_folder, f"batch{batch_idx}_img{j}_comparison.png")
        cv2.imwrite(filename, combined)
